{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready complete script (Option B: OpenCV-based weather augmentations)\n",
        "# Run in Google Colab with GPU runtime.\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1) Environment setup (keep minimal; avoid albumentations/imgaug)\n",
        "# -----------------------------------------------------------------------------\n",
        "!nvidia-smi || true\n",
        "!pip install --quiet editdistance wget matplotlib opencv-python\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2) Imports\n",
        "# -----------------------------------------------------------------------------\n",
        "import os, sys, time, math, random, shutil, zipfile, glob\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import editdistance\n",
        "import wget\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3) Utilities\n",
        "# -----------------------------------------------------------------------------\n",
        "def mkdir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def download_and_extract(url, out_dir, filename=None):\n",
        "    mkdir(out_dir)\n",
        "    if filename is None:\n",
        "        filename = os.path.basename(url)\n",
        "    out_path = os.path.join(out_dir, filename)\n",
        "    if not os.path.exists(out_path):\n",
        "        print(\"Downloading\", url)\n",
        "        try:\n",
        "            wget.download(url, out=out_path)\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(\"Download failed:\", e)\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Already downloaded:\", out_path)\n",
        "    # try unzip\n",
        "    try:\n",
        "        if out_path.endswith(\".zip\"):\n",
        "            print(\"Extracting\", out_path)\n",
        "            with zipfile.ZipFile(out_path, 'r') as z:\n",
        "                z.extractall(out_dir)\n",
        "    except Exception as e:\n",
        "        print(\"Extraction error (maybe tar.gz?)\", e)\n",
        "    return out_path\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4) Attempt to download IIIT5K and SVT (optional). If download fails, upload dataset to DATA_ROOT.\n",
        "# -----------------------------------------------------------------------------\n",
        "DATA_ROOT = \"/content/text_rec_data\"\n",
        "mkdir(DATA_ROOT)\n",
        "\n",
        "iiit_url = \"https://cvit.iiit.ac.in/research/projects/cvit-projects/scene-text-recognition/data/IIIT5KWord_V3.zip\"\n",
        "svt_url  = \"https://vision.ucsd.edu/~kai/svt/SVT.zip\"\n",
        "\n",
        "# Try downloads (if links broken, user can upload datasets to DATA_ROOT)\n",
        "try:\n",
        "    download_and_extract(iiit_url, DATA_ROOT)\n",
        "except Exception as e:\n",
        "    print(\"IIIT download attempt failed:\", e)\n",
        "try:\n",
        "    download_and_extract(svt_url, DATA_ROOT)\n",
        "except Exception as e:\n",
        "    print(\"SVT download attempt failed:\", e)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5) Locate image-label pairs (flexible parser)\n",
        "# -----------------------------------------------------------------------------\n",
        "def find_images_and_labels(root):\n",
        "    pairs = []\n",
        "    root = Path(root)\n",
        "    # find common annotation .txt files\n",
        "    txt_files = list(root.rglob(\"*.txt\"))\n",
        "    for tf in txt_files:\n",
        "        name = tf.name.lower()\n",
        "        if \"train\" in name or \"test\" in name or \"gt\" in name or \"label\" in name:\n",
        "            try:\n",
        "                with open(tf, \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
        "                    lines = [l.strip() for l in f if l.strip()]\n",
        "                for l in lines:\n",
        "                    parts = l.split()\n",
        "                    if len(parts) >= 2:\n",
        "                        imgname = parts[0]\n",
        "                        word = parts[1]\n",
        "                        # Locate image file near tf or in root\n",
        "                        candidate_paths = [\n",
        "                            tf.parent / imgname,\n",
        "                            root / imgname,\n",
        "                            tf.parent / (imgname + \".jpg\"),\n",
        "                            root / (imgname + \".jpg\"),\n",
        "                            tf.parent / (imgname + \".png\"),\n",
        "                            root / (imgname + \".png\"),\n",
        "                        ]\n",
        "                        found = False\n",
        "                        for c in candidate_paths:\n",
        "                            if c.exists():\n",
        "                                pairs.append((str(c), word))\n",
        "                                found = True\n",
        "                                break\n",
        "                        # if not found, attempt to find any file whose stem matches imgname\n",
        "                        if not found:\n",
        "                            for ext in [\".jpg\",\".png\",\".jpeg\",\".JPG\"]:\n",
        "                                cand = tf.parent / (imgname + ext)\n",
        "                                if cand.exists():\n",
        "                                    pairs.append((str(cand), word))\n",
        "                                    found = True\n",
        "                                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "    # fallback: if none found, try to use image files with .txt sidecar files\n",
        "    if len(pairs) == 0:\n",
        "        for img in root.rglob(\"*.*\"):\n",
        "            if img.suffix.lower() in [\".jpg\",\".png\",\".jpeg\"]:\n",
        "                lab = img.with_suffix(\".txt\")\n",
        "                if lab.exists():\n",
        "                    try:\n",
        "                        w = lab.read_text().strip().splitlines()[0].strip()\n",
        "                        pairs.append((str(img), w))\n",
        "                    except:\n",
        "                        continue\n",
        "    return pairs\n",
        "\n",
        "pairs = find_images_and_labels(DATA_ROOT)\n",
        "print(\"Found annotation pairs:\", len(pairs))\n",
        "if len(pairs) == 0:\n",
        "    print(\"No pairs found automatically. Upload datasets (IIIT5K / SVT) into /content/text_rec_data and re-run.\")\n",
        "\n",
        "# Trim to reasonable size for Colab if very large\n",
        "MAX_SAMPLES = 20000\n",
        "if len(pairs) > MAX_SAMPLES:\n",
        "    random.shuffle(pairs)\n",
        "    pairs = pairs[:MAX_SAMPLES]\n",
        "    print(\"Sampling down to\", len(pairs), \"pairs.\")\n",
        "\n",
        "# Train/Val/Test split\n",
        "random.shuffle(pairs)\n",
        "n = len(pairs)\n",
        "ntrain = int(0.8 * n)\n",
        "nval = int(0.1 * n)\n",
        "train_pairs = pairs[:ntrain]\n",
        "val_pairs = pairs[ntrain:ntrain+nval]\n",
        "test_pairs = pairs[ntrain+nval:]\n",
        "print(\"Train/Val/Test:\", len(train_pairs), len(val_pairs), len(test_pairs))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6) Character vocabulary\n",
        "# -----------------------------------------------------------------------------\n",
        "alphabet = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!@#$%^&*()-_+=/:;.,?'\\\" \"\n",
        "char_to_idx = {c: i+1 for i, c in enumerate(alphabet)}  # 0 reserved for blank (CTC)\n",
        "idx_to_char = {i+1: c for i, c in enumerate(alphabet)}\n",
        "\n",
        "def encode_str_to_tensor(s):\n",
        "    seq = []\n",
        "    for ch in s:\n",
        "        if ch in char_to_idx:\n",
        "            seq.append(char_to_idx[ch])\n",
        "        elif ch.lower() in char_to_idx:\n",
        "            seq.append(char_to_idx[ch.lower()])\n",
        "    if len(seq) == 0:\n",
        "        seq = [char_to_idx.get(\" \",1)]\n",
        "    return torch.LongTensor(seq)\n",
        "\n",
        "def decode_seq(indices):\n",
        "    # collapse repeats and remove blanks (0)\n",
        "    res = []\n",
        "    prev = None\n",
        "    for idx in indices:\n",
        "        if idx == 0:\n",
        "            prev = None\n",
        "            continue\n",
        "        if idx == prev:\n",
        "            continue\n",
        "        prev = idx\n",
        "        res.append(idx_to_char.get(int(idx), \"\"))\n",
        "    return \"\".join(res)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 7) OpenCV-based weather augmentation functions (no external libs)\n",
        "# -----------------------------------------------------------------------------\n",
        "def add_snow(img, snow_strength=0.04):\n",
        "    h, w = img.shape[:2]\n",
        "    snow = (np.random.rand(h, w) < snow_strength).astype(np.uint8) * 255\n",
        "    snow = cv2.blur(snow, (3,3))\n",
        "    snow = np.expand_dims(snow, 2)\n",
        "    img = img.astype(np.float32)\n",
        "    img = np.clip(img + snow, 0, 255)\n",
        "    return img.astype(np.uint8)\n",
        "\n",
        "def add_rain(img, drop_length=15, drop_width=1, drop_count=400, slant=10):\n",
        "    h, w = img.shape[:2]\n",
        "    rain = np.zeros((h, w), dtype=np.uint8)\n",
        "    for _ in range(drop_count):\n",
        "        x = random.randint(0, w-1)\n",
        "        y = random.randint(0, h-1)\n",
        "        x2 = min(w-1, x + slant)\n",
        "        y2 = min(h-1, y + drop_length)\n",
        "        cv2.line(rain, (x,y), (x2,y2), 255, drop_width)\n",
        "    rain = cv2.blur(rain, (3,3))\n",
        "    rain = np.expand_dims(rain, 2)\n",
        "    img = img.astype(np.float32)\n",
        "    img = np.clip(img + rain, 0, 255)\n",
        "    # motion blur to emulate streaks\n",
        "    ksize = random.choice([3,5,7])\n",
        "    kernel = np.zeros((ksize, ksize))\n",
        "    kernel[int((ksize-1)/2), :] = np.ones(ksize)\n",
        "    kernel = kernel / ksize\n",
        "    img = cv2.filter2D(img, -1, kernel)\n",
        "    return img.astype(np.uint8)\n",
        "\n",
        "def add_fog(img, fog_strength=0.06):\n",
        "    h, w = img.shape[:2]\n",
        "    fog = np.full((h, w, 3), 255, dtype=np.uint8)\n",
        "    alpha = np.random.uniform(0.02, fog_strength)\n",
        "    img = img.astype(np.float32)\n",
        "    out = cv2.addWeighted(img, 1-alpha, fog.astype(np.float32), alpha, 0)\n",
        "    return out.astype(np.uint8)\n",
        "\n",
        "def random_weather_augment(img, img_h=32, img_w=128):\n",
        "    # input img: RGB uint8\n",
        "    img_copy = img.copy()\n",
        "    p = random.random()\n",
        "    if p < 0.18:\n",
        "        img_copy = add_rain(img_copy,\n",
        "                            drop_length=random.randint(10,25),\n",
        "                            drop_count=random.randint(150,700),\n",
        "                            slant=random.randint(-10,10))\n",
        "    elif p < 0.30:\n",
        "        img_copy = add_snow(img_copy, snow_strength=random.uniform(0.01, 0.06))\n",
        "    elif p < 0.44:\n",
        "        img_copy = add_fog(img_copy, fog_strength=random.uniform(0.04, 0.18))\n",
        "    # brightness / contrast\n",
        "    if random.random() < 0.5:\n",
        "        alpha = random.uniform(0.7, 1.4)  # contrast\n",
        "        beta = random.uniform(-30, 30)    # brightness\n",
        "        img_copy = cv2.convertScaleAbs(img_copy, alpha=alpha, beta=beta)\n",
        "    # blur occasionally\n",
        "    if random.random() < 0.3:\n",
        "        k = random.choice([3,5,7])\n",
        "        img_copy = cv2.GaussianBlur(img_copy, (k,k), 0)\n",
        "    # JPEG compression artifact simulation\n",
        "    if random.random() < 0.25:\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), random.randint(30,90)]\n",
        "        _, encimg = cv2.imencode('.jpg', img_copy, encode_param)\n",
        "        img_copy = cv2.imdecode(encimg, cv2.IMREAD_COLOR)\n",
        "    img_copy = cv2.resize(img_copy, (img_w, img_h))\n",
        "    return img_copy\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 8) Dataset and collate\n",
        "# -----------------------------------------------------------------------------\n",
        "class WordDataset(Dataset):\n",
        "    def __init__(self, pairs, img_h=32, img_w=128, augment=False):\n",
        "        self.pairs = pairs\n",
        "        self.img_h = img_h\n",
        "        self.img_w = img_w\n",
        "        self.augment = augment\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.pairs[idx]\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            img = np.zeros((self.img_h, self.img_w, 3), dtype=np.uint8)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if self.augment:\n",
        "            try:\n",
        "                img = random_weather_augment(img, self.img_h, self.img_w)\n",
        "            except Exception:\n",
        "                img = cv2.resize(img, (self.img_w, self.img_h)).astype(np.uint8)\n",
        "        else:\n",
        "            img = cv2.resize(img, (self.img_w, self.img_h)).astype(np.uint8)\n",
        "        # grayscale and normalize to [-1,1]\n",
        "        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY).astype(np.float32) / 255.0\n",
        "        img_gray = np.expand_dims(img_gray, axis=0)  # C,H,W\n",
        "        img_t = (torch.FloatTensor(img_gray) - 0.5) / 0.5\n",
        "        # encode label\n",
        "        label_enc = []\n",
        "        for ch in label:\n",
        "            if ch in char_to_idx:\n",
        "                label_enc.append(char_to_idx[ch])\n",
        "            elif ch.lower() in char_to_idx:\n",
        "                label_enc.append(char_to_idx[ch.lower()])\n",
        "        if len(label_enc) == 0:\n",
        "            label_enc = [char_to_idx.get(\" \",1)]\n",
        "        label_enc = torch.LongTensor(label_enc)\n",
        "        label_len = label_enc.numel()\n",
        "        return img_t, label_enc, label_len, label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    label_lens = torch.LongTensor([item[2] for item in batch])\n",
        "    raw_labels = [item[3] for item in batch]\n",
        "    imgs_t = torch.stack(imgs)\n",
        "    concat = torch.cat(labels) if len(labels)>0 else torch.LongTensor([])\n",
        "    return imgs_t, concat, label_lens, raw_labels\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 9) CRNN model (CNN -> BiLSTM -> CTC)\n",
        "# -----------------------------------------------------------------------------\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, nIn, nHidden, nOut):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
        "        self.embedding = nn.Linear(nHidden*2, nOut)\n",
        "    def forward(self, input):\n",
        "        recurrent, _ = self.rnn(input)\n",
        "        T, b, h = recurrent.size()\n",
        "        t_rec = recurrent.view(T*b, h)\n",
        "        output = self.embedding(t_rec)\n",
        "        output = output.view(T, b, -1)\n",
        "        return output\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, imgH, nc, nclass, nh):\n",
        "        super(CRNN, self).__init__()\n",
        "        assert imgH % 16 == 0\n",
        "        ks = [3,3,3,3,3,3,2]\n",
        "        ps = [1,1,1,1,1,1,0]\n",
        "        ss = [1,1,1,1,1,1,1]\n",
        "        nm = [64,128,256,256,512,512,512]\n",
        "        cnn = nn.Sequential()\n",
        "        def convRelu(i, batch_norm=False):\n",
        "            nIn = nc if i==0 else nm[i-1]\n",
        "            nOut = nm[i]\n",
        "            cnn.add_module('conv{0}'.format(i), nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
        "            if batch_norm:\n",
        "                cnn.add_module('bn{0}'.format(i), nn.BatchNorm2d(nOut))\n",
        "            cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
        "        convRelu(0)\n",
        "        cnn.add_module('pooling0', nn.MaxPool2d(2,2))\n",
        "        convRelu(1)\n",
        "        cnn.add_module('pooling1', nn.MaxPool2d(2,2))\n",
        "        convRelu(2, batch_norm=True)\n",
        "        convRelu(3)\n",
        "        cnn.add_module('pooling2', nn.MaxPool2d((2,1),(2,1),(0,0)))\n",
        "        convRelu(4, batch_norm=True)\n",
        "        convRelu(5)\n",
        "        cnn.add_module('pooling3', nn.MaxPool2d((2,1),(2,1),(0,0)))\n",
        "        convRelu(6, batch_norm=True)\n",
        "        self.cnn = cnn\n",
        "        self.rnn = nn.Sequential(\n",
        "            BidirectionalLSTM(512, nh, nh),\n",
        "            BidirectionalLSTM(nh, nh, nclass)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        conv = self.cnn(x)\n",
        "        b, c, h, w = conv.size()\n",
        "        conv = conv.squeeze(2)  # B, C, W\n",
        "        conv = conv.permute(2,0,1)  # W, B, C\n",
        "        output = self.rnn(conv)\n",
        "        return output\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 10) Training and evaluation helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    for imgs, labels_concat, label_lens, raw_labels in dataloader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels_concat = labels_concat.to(device)\n",
        "        label_lens = label_lens.to(device)\n",
        "        batch_size = imgs.size(0)\n",
        "        preds = model(imgs)  # T, B, C\n",
        "        preds_log_softmax = F.log_softmax(preds, dim=2)\n",
        "        preds_len = torch.IntTensor([preds.size(0)] * batch_size)\n",
        "        loss = criterion(preds_log_softmax, labels_concat, preds_len, label_lens)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        count += 1\n",
        "    return total_loss / max(count,1)\n",
        "\n",
        "def greedy_decode(preds):\n",
        "    _, maxidx = preds.max(2)\n",
        "    maxidx = maxidx.transpose(1,0).cpu().numpy()  # B, T\n",
        "    results = []\n",
        "    for row in maxidx:\n",
        "        res = []\n",
        "        prev = -1\n",
        "        for ch in row:\n",
        "            if ch != prev and ch != 0:\n",
        "                res.append(idx_to_char.get(int(ch), ''))\n",
        "            prev = ch\n",
        "        results.append(\"\".join(res))\n",
        "    return results\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_cer = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels_concat, label_lens, raw_labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            batch_size = imgs.size(0)\n",
        "            preds = model(imgs)\n",
        "            preds_log = F.log_softmax(preds, dim=2)\n",
        "            decoded = greedy_decode(preds_log)\n",
        "            for p, g in zip(decoded, raw_labels):\n",
        "                total += 1\n",
        "                if p.strip().lower() == g.strip().lower():\n",
        "                    correct += 1\n",
        "                total_cer += editdistance.eval(p, g) / max(1, len(g))\n",
        "    word_acc = correct / total if total>0 else 0.0\n",
        "    avg_cer = total_cer / total if total>0 else 0.0\n",
        "    return word_acc, avg_cer\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 11) Create datasets / loaders\n",
        "# -----------------------------------------------------------------------------\n",
        "IMG_H = 32\n",
        "IMG_W = 128\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_ds = WordDataset(train_pairs, img_h=IMG_H, img_w=IMG_W, augment=True)\n",
        "val_ds   = WordDataset(val_pairs, img_h=IMG_H, img_w=IMG_W, augment=False)\n",
        "test_ds  = WordDataset(test_pairs, img_h=IMG_H, img_w=IMG_W, augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "print(\"Train/Val/Test dataset sizes:\", len(train_ds), len(val_ds), len(test_ds))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 12) Instantiate model, loss, optimizer\n",
        "# -----------------------------------------------------------------------------\n",
        "nclass = len(alphabet) + 1\n",
        "nh = 256\n",
        "model = CRNN(IMG_H, 1, nclass, nh).to(device)\n",
        "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "print(model)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 13) Training loop\n",
        "# -----------------------------------------------------------------------------\n",
        "EPOCHS = 20\n",
        "best_val_acc = 0.0\n",
        "history = {\"train_loss\": [], \"val_acc\": [], \"val_cer\": []}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_acc, val_cer = evaluate(model, val_loader)\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "    history[\"val_cer\"].append(val_cer)\n",
        "    scheduler.step()\n",
        "    t1 = time.time()\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  train_loss: {train_loss:.4f}  val_acc: {val_acc*100:.2f}%  val_cer: {val_cer:.3f}  time: {t1-t0:.1f}s\")\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\"model_state\": model.state_dict(), \"alphabet\": alphabet}, \"/content/best_crnn.pth\")\n",
        "        print(\"Saved best model.\")\n",
        "\n",
        "torch.save({\"model_state\": model.state_dict(), \"alphabet\": alphabet}, \"/content/final_crnn.pth\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 14) Plot validation accuracy vs epochs and training loss\n",
        "# -----------------------------------------------------------------------------\n",
        "if len(history[\"val_acc\"])>0:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(range(1,1+len(history[\"val_acc\"])), [x*100 for x in history[\"val_acc\"]], marker='o')\n",
        "    plt.title(\"Validation Word Accuracy vs Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Val Word Accuracy (%)\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if len(history[\"train_loss\"])>0:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(range(1,1+len(history[\"train_loss\"])), history[\"train_loss\"], marker='o')\n",
        "    plt.title(\"Train Loss vs Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Train Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 15) Final evaluation and sample predictions\n",
        "# -----------------------------------------------------------------------------\n",
        "if os.path.exists(\"/content/best_crnn.pth\"):\n",
        "    best = torch.load(\"/content/best_crnn.pth\", map_location=device)\n",
        "    model.load_state_dict(best[\"model_state\"])\n",
        "\n",
        "test_acc, test_cer = evaluate(model, test_loader)\n",
        "print(f\"Final Test word accuracy: {test_acc*100:.2f}%, Test CER: {test_cer:.3f}\")\n",
        "\n",
        "# Show sample predictions\n",
        "model.eval()\n",
        "samples = 12\n",
        "count = 0\n",
        "plt.figure(figsize=(12,6))\n",
        "for imgs, labels_concat, label_lens, raw_labels in test_loader:\n",
        "    imgs_cpu = imgs\n",
        "    imgs = imgs.to(device)\n",
        "    with torch.no_grad():\n",
        "        preds = model(imgs)\n",
        "        decoded = greedy_decode(F.log_softmax(preds, dim=2))\n",
        "    for i in range(len(decoded)):\n",
        "        if count >= samples: break\n",
        "        img = imgs_cpu[i].cpu().numpy().transpose(1,2,0)\n",
        "        img = (img * 0.5) + 0.5\n",
        "        img = img.squeeze()\n",
        "        plt.subplot(3,4,count+1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"P: {decoded[i]}\\nG: {raw_labels[i]}\")\n",
        "        plt.axis('off')\n",
        "        count += 1\n",
        "    if count >= samples:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Best model saved to: /content/best_crnn.pth\")\n",
        "print(\"Final model saved to: /content/final_crnn.pth\")\n",
        "print(\"\\nNotes:\")\n",
        "print(\"- This version uses OpenCV-only augmentations to simulate weather (rain, snow, fog) and related artifacts.\")\n",
        "print(\"- If dataset downloads failed, upload IIIT5K / SVT (or any scene text dataset) into /content/text_rec_data\")\n",
        "print(\"- For better accuracy, pretrain on large synthetic datasets (MJ, SynthText) and use attention decoders or beam search.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2DsPJyy2CIHE",
        "outputId": "73897a5f-dd39-4179-d8cc-7490612869a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 24 18:04:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-683670041.py\", line 17, in <cell line: 0>\n",
            "    import cv2\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\", line 54, in create_module\n",
            "    loader.exec_module(module)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\", line 181, in <module>\n",
            "    bootstrap()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\", line 153, in bootstrap\n",
            "    native_module = importlib.import_module(\"cv2\")\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\", line 51, in create_module\n",
            "    module = importlib.util.module_from_spec(spec)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-683670041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_code_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreviously_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0mbootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\u001b[0m in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mpy_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mnative_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}